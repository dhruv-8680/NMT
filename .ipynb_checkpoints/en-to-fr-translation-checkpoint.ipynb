{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T18:55:11.655994Z",
     "start_time": "2020-02-12T18:55:04.898896Z"
    },
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhruv/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/dhruv/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/dhruv/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/dhruv/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/dhruv/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/dhruv/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/dhruv/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f89b3b8df6d1b5907a2694f6a8156d362d6a5673"
   },
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "#     print(type(data))\n",
    "    return data.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "694c6255f63038978f569512c0f462c8b5743e0e"
   },
   "outputs": [],
   "source": [
    "source_path = \".nmt-eng-to-fr/small_vocab_en\"\n",
    "target_path = \".nmt-eng-to-fr/small_vocab_fr\"\n",
    "_source_data = str.join(\"\\n\",load_data(source_path).split(\"\\n\")[0:100000])\n",
    "_target_data = str.join(\"\\n\",load_data(target_path).split(\"\\n\")[0:100000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "63851e17137c49ced6a961554ed997a2ae47fffa"
   },
   "outputs": [],
   "source": [
    "source_data=\"\"\n",
    "target_data=\"\"\n",
    "for idx, sentence in enumerate(_source_data.split(\"\\n\")):\n",
    "    if len(sentence.split()) < 50 and len(_target_data.split(\"\\n\")[idx])<50:\n",
    "        print(idx)\n",
    "        source_data+=sentence+\"\\n\"\n",
    "        target_data+=_target_data.split(\"\\n\")[idx]+\"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "13d94ef084f24f5240d75de95e231387a5107ea2"
   },
   "outputs": [],
   "source": [
    "print(\"Total sentences in Source Data : \",len(source_data.split(\"\\n\")))\n",
    "print(\"Total sentences in Target Data : \",len(target_data.split(\"\\n\")))\n",
    "print(\"Average words in source data : \", np.average([len(sentence.split()) for sentence in source_data.split(\"\\n\")]))\n",
    "print(\"Average words in target data : \", np.average([len(sentence.split()) for sentence in target_data.split(\"\\n\")]))\n",
    "print(\"Max words in source data : \", max([len(sentence.split()) for sentence in source_data.split(\"\\n\")]))\n",
    "print(\"Max words in Target data : \", max([len(sentence.split()) for sentence in target_data.split(\"\\n\")]))\n",
    "print(\"Min words in source data : \", min([len(sentence.split()) for sentence in source_data.split(\"\\n\")]))\n",
    "print(\"Min words in Target data : \", min([len(sentence.split()) for sentence in target_data.split(\"\\n\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6b18f6e09babd2331f2e5fd3e1a648f1acab2075"
   },
   "outputs": [],
   "source": [
    "#Def Dictionary\n",
    "def create_dictionary(corpus):\n",
    "    dictionary = {'<PAD>': 0, '<EOS>': 1, '<UNK>': 2, '<GO>': 3 }\n",
    "    vocab = list(set(corpus))\n",
    "    for word in vocab:\n",
    "        dictionary[word]=len(dictionary)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, reverse_dictionary, len(dictionary)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3191de1072e0e79ef359fb6b1d302d0e2e6b4f75"
   },
   "outputs": [],
   "source": [
    "source_word_to_int, source_int_to_word, source_vocab_size = create_dictionary(source_data.split())\n",
    "target_word_to_int, target_int_to_word, target_vocab_size = create_dictionary(target_data.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "23768519b5dff36a101d99b988f665686a55b70a"
   },
   "outputs": [],
   "source": [
    "print(source_vocab_size)\n",
    "print(target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4c7b6410c0bcf1e504590bbe915e10df3d521fc5"
   },
   "outputs": [],
   "source": [
    "def text_to_int(corpus, dictionary, isTarget=False):\n",
    "    sentences = corpus.split(\"\\n\")\n",
    "    converted_data=[]\n",
    "    for sentence in sentences:\n",
    "        temp_converted = []\n",
    "        for word in sentence.split():\n",
    "            temp_converted.append(dictionary[str(word)])\n",
    "        if isTarget:\n",
    "            temp_converted.append(dictionary[str(\"<EOS>\")])\n",
    "        converted_data.append(temp_converted)\n",
    "    return np.array(converted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "31930336981a84b2b92b46a3df634bc03dc87153"
   },
   "outputs": [],
   "source": [
    "#Add Padding to data\n",
    "def add_padding_in_batch(data, start_index, end_index, dictionary):\n",
    "    batch = data[start_index:end_index]\n",
    "#     print(batch)\n",
    "    max_words_in_batch = max([len(sentence) for sentence in batch])\n",
    "#     print(max_words_in_batch)\n",
    "    created_batch=[]\n",
    "    batch_length = []\n",
    "    for sentence in batch:\n",
    "        padding_list = [dictionary[str(\"<PAD>\")] for _ in range(max_words_in_batch - len(sentence))]\n",
    "#         print(sentence+padding_list)\n",
    "        created_batch.append(sentence+padding_list)\n",
    "        batch_length.append(int(len(sentence+padding_list)))\n",
    "    return created_batch,batch_length\n",
    "\n",
    "def reverse_encoder_data(data):\n",
    "    return np.array(data)[:,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a2e6c8a3f233cd51115af34ed396dbfc3cfbdf5e"
   },
   "outputs": [],
   "source": [
    "def get_accuracy(target, logits):\n",
    "    \"\"\"\n",
    "    Calculate accuracy\n",
    "    \"\"\"\n",
    "    max_seq = max(target.shape[1], logits.shape[1])\n",
    "    if max_seq - target.shape[1]:\n",
    "        target = np.pad(\n",
    "            target,\n",
    "            [(0,0),(0,max_seq - target.shape[1])],\n",
    "            'constant')\n",
    "    if max_seq - logits.shape[1]:\n",
    "        logits = np.pad(\n",
    "            logits,\n",
    "            [(0,0),(0,max_seq - logits.shape[1])],\n",
    "            'constant')\n",
    "\n",
    "    return np.mean(np.equal(target, logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ff950ba694dc94e44c646f3219c68547d7b5833b"
   },
   "outputs": [],
   "source": [
    "source_processed_corpus = text_to_int(source_data, source_word_to_int)\n",
    "target_processed_corpus = text_to_int(target_data, target_word_to_int, isTarget=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "722435d4f9691d45d0e263cd041bd739ccec4816"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(source_processed_corpus,target_processed_corpus,\n",
    "                                                    test_size=0.10, \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aba729ee7c0a45fafbefd46047c1e2a9571cf5c2"
   },
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "embedding_size = 200\n",
    "output_size = target_vocab_size\n",
    "batch_size=128\n",
    "dataset_size = len(source_data.split(\"\\n\"))\n",
    "keep_prob = 0.5\n",
    "n_layers = 2\n",
    "n_neurons = 512\n",
    "training_steps = 15\n",
    "l_rate = 0.001\n",
    "target_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e70d090eb395c1c71944f4d9b8447076d71551a2"
   },
   "outputs": [],
   "source": [
    "#input placeholder\n",
    "tf.reset_default_graph()\n",
    "encoder_input = tf.placeholder(tf.int32, shape=[None, None], name=\"question_input\")\n",
    "target = tf.placeholder(tf.int32, shape=[None, None], name=\"answer\")\n",
    "\n",
    "keep_ratio = tf.placeholder(tf.float32, name=\"keep_ratio\")\n",
    "target_length = tf.placeholder(tf.int32, shape=[None], name=\"target_length\")\n",
    "max_target_length = tf.reduce_max(input_tensor=target_length, name=\"max_target_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bdf1de3f9bb48ebfb7a32ab59008a7d590fc086c"
   },
   "outputs": [],
   "source": [
    "#Encoder Cell\n",
    "def encoder(inputs, n_neurons, keep_ratio, embedding_size, vocab_size, n_layers):\n",
    "    #Create Word Embeddings\n",
    "    embeddings = tf.contrib.layers.embed_sequence(ids=inputs, embed_dim=embedding_size, vocab_size=vocab_size)\n",
    "    print(embeddings)\n",
    "    \n",
    "    #Create encoder_cell\n",
    "    encoder_cell = [tf.nn.rnn_cell.LSTMCell(num_units=n_neurons) for _ in range(n_layers)]\n",
    "    encoder_cell = tf.nn.rnn_cell.MultiRNNCell(cells=encoder_cell)\n",
    "    encoder_cell = tf.nn.rnn_cell.DropoutWrapper(cell=encoder_cell, output_keep_prob=keep_ratio)\n",
    "    \n",
    "    #Encoder_Unrolling\n",
    "    outputs, states = tf.nn.dynamic_rnn(cell=encoder_cell, inputs=embeddings, dtype=tf.float32)\n",
    "    return outputs, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f2a9177ed62c9b6b78a2b525064a77c444f3de8b"
   },
   "outputs": [],
   "source": [
    "#Attention Mechanism\n",
    "def attention_mechanism(n_neurons, encoder_outputs, decoder_cell, batch_size, encoder_states):\n",
    "    attention_mechanism = tf.contrib.seq2seq.LuongAttention(memory=encoder_outputs, num_units=n_neurons)\n",
    "    decoder_cell = tf.contrib.seq2seq.AttentionWrapper(cell=decoder_cell, attention_mechanism=attention_mechanism, attention_layer_size=n_neurons) \n",
    "    initial_state = decoder_cell.zero_state(batch_size=batch_size, dtype=tf.float32).clone(cell_state=encoder_states)\n",
    "    return decoder_cell, initial_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5e156c1e8198bbfe87ca9cd500bbbb10a751a49a"
   },
   "outputs": [],
   "source": [
    "#Decoder Word Embeddings\n",
    "def process_decoder_inputs(inputs, target_vocab_size, embedding_size, start_token, batch_size):\n",
    "    #Add SOS tag in the input\n",
    "    sliced_input = tf.strided_slice(input_=inputs, begin=(0,0), end=(batch_size, -1), strides=(1,1))\n",
    "    sliced_input = tf.concat([tf.fill(dims=[batch_size,1],value=start_token),sliced_input], 1, name=\"decoder_input\")\n",
    "    print(sliced_input)\n",
    "    return sliced_input\n",
    "\n",
    "#Decoder Embeddings\n",
    "def create_decoder_embeddings(inputs, target_vocab_size, embedding_size):\n",
    "    embedding_weights = tf.get_variable(name=\"decoder_embedding_weights\", shape=[target_vocab_size, embedding_size], initializer=tf.random_normal_initializer())\n",
    "    print(embedding_weights)\n",
    "    embeddings = tf.nn.embedding_lookup(ids=inputs, params=embedding_weights, name=\"decoder_embedding\")\n",
    "    print(embeddings)\n",
    "    return embedding_weights, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "96eaa64cb2d5952d234b78b6a2c469d41bca761a"
   },
   "outputs": [],
   "source": [
    "#Decoder Cell\n",
    "def create_decoder_cell(n_neurons, n_layers, keep_ratio):\n",
    "    #Decoder Cell\n",
    "    decoder_cell = [tf.nn.rnn_cell.LSTMCell(num_units=n_neurons, name=\"decoder_cell\") for _ in range(n_layers)]\n",
    "    decoder_cell = tf.nn.rnn_cell.MultiRNNCell(decoder_cell)\n",
    "    decoder_cell = tf.nn.rnn_cell.DropoutWrapper(decoder_cell, output_keep_prob=keep_ratio)\n",
    "    \n",
    "    return decoder_cell\n",
    "\n",
    "#Create Training Helper\n",
    "def create_training_helper(inputs, target_seq_length):\n",
    "    #Decoder Helper\n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(inputs=inputs, sequence_length=target_seq_length, name=\"decoder_helper\")\n",
    "    return helper\n",
    "\n",
    "    #Attention Mechanism\n",
    "#     decoder_cell, initial_states = attention_mechanism(batch_size=batch_size, \n",
    "#                                                        decoder_cell=decoder_cell, \n",
    "#                                                        encoder_outputs=encoder_outputs, \n",
    "#                                                        encoder_states=initial_states, \n",
    "#                                                        n_neurons=n_neurons)\n",
    "    \n",
    "def create_basic_decoder(decoder_cell, helper, initial_states, projection_layer):\n",
    "    #Basic Decoder\n",
    "    basic_decoder = tf.contrib.seq2seq.BasicDecoder(cell=decoder_cell, \n",
    "                                                    helper=helper, \n",
    "                                                    initial_state=initial_states, \n",
    "                                                    output_layer=projection_layer)\n",
    "    \n",
    "    #basic decoder dynamic unrolling\n",
    "    final_decoder_outputs, final_decoder_states, final_decoder_sequence_length = tf.contrib.seq2seq.dynamic_decode(decoder=basic_decoder)\n",
    "    print(final_decoder_outputs)\n",
    "    print(final_decoder_states)\n",
    "    print(initial_states)\n",
    "    print(decoder_cell)\n",
    "    return final_decoder_outputs, final_decoder_states\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b2d8bfc46eb17200a5d6db4bbe8b6bbe0011baf0"
   },
   "outputs": [],
   "source": [
    "def inference(decoder_cell, decoder_embedding_weights, start_token, end_token, batch_size, projection_layer, max_sequence_length, initial_state):\n",
    "    #Greedy Helper\n",
    "    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embedding=decoder_embedding_weights, start_tokens=tf.fill(dims=[batch_size], value=start_token), end_token=end_token)\n",
    "    \n",
    "    #basic decoder\n",
    "    basic_decoder = tf.contrib.seq2seq.BasicDecoder(cell=decoder_cell, helper=helper, initial_state=initial_state, output_layer=projection_layer)\n",
    "    \n",
    "    #dynamic unrolling\n",
    "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder=basic_decoder, impute_finished=True, maximum_iterations=max_sequence_length)\n",
    "    print(outputs)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a4bb0ca5fddd5507c5f9fe6fd350e6eb08e1d0a5"
   },
   "outputs": [],
   "source": [
    "encoder_outputs, encoder_states = encoder(inputs=encoder_input, \n",
    "                                          embedding_size=embedding_size, \n",
    "                                          keep_ratio=keep_prob, \n",
    "                                          n_layers=n_layers, \n",
    "                                          n_neurons=n_neurons,\n",
    "                                          vocab_size=source_vocab_size)\n",
    "\n",
    "processed_decoder_inputs = process_decoder_inputs(inputs=target, \n",
    "                                                  batch_size=batch_size, \n",
    "                                                  embedding_size=embedding_size, \n",
    "                                                  start_token=target_word_to_int[\"<GO>\"], \n",
    "                                                  target_vocab_size=target_vocab_size)\n",
    "\n",
    "decoder_embedding_weights, decoder_embeddings = create_decoder_embeddings(inputs=processed_decoder_inputs, \n",
    "                                                                          embedding_size=embedding_size, \n",
    "                                                                          target_vocab_size=target_vocab_size)\n",
    "\n",
    "decoder_cell = create_decoder_cell(n_layers=n_layers, \n",
    "                                   keep_ratio=keep_ratio, \n",
    "                                   n_neurons=n_neurons)\n",
    "\n",
    "helper = create_training_helper(inputs=decoder_embeddings, target_seq_length=target_length)\n",
    "\n",
    "projection_layer = tf.layers.Dense(units=output_size)\n",
    "\n",
    "decoder_cell, initial_states = attention_mechanism(decoder_cell=decoder_cell,\n",
    "                                                   batch_size=batch_size, \n",
    "                                                   n_neurons=n_neurons, \n",
    "                                                   encoder_states=encoder_states, \n",
    "                                                   encoder_outputs=encoder_outputs)\n",
    "\n",
    "decoder_outputs, decoder_states = create_basic_decoder(decoder_cell=decoder_cell, \n",
    "                                                       helper=helper, \n",
    "                                                       initial_states=initial_states, \n",
    "                                                       projection_layer=projection_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eed2e3b21b033a558591aa6c97b633112c4e0f01"
   },
   "outputs": [],
   "source": [
    "#Inference Layer\n",
    "infer_outputs = inference(batch_size=batch_size, \n",
    "                          decoder_cell=decoder_cell, \n",
    "                          decoder_embedding_weights=decoder_embedding_weights, \n",
    "                          start_token=target_word_to_int[\"<GO>\"], \n",
    "                          end_token=target_word_to_int[\"<EOS>\"], \n",
    "                          initial_state=initial_states, \n",
    "                          max_sequence_length=max_target_length, \n",
    "                          projection_layer=projection_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0adfc81ad08b5ab227938a2fe531530c947e6394"
   },
   "outputs": [],
   "source": [
    "logits = decoder_outputs.rnn_output\n",
    "predictions = infer_outputs.sample_id\n",
    "print(logits)\n",
    "print(target)\n",
    "print(predictions)\n",
    "# test = tf.argmax(logits,axis=2)\n",
    "# print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4cb62dc22a2f684d1a08e290580942311d22e4c3"
   },
   "outputs": [],
   "source": [
    "#Sequence Masking\n",
    "mask = tf.sequence_mask(lengths=target_length, maxlen=max_target_length, dtype=tf.float32, name=\"mask\")\n",
    "loss = tf.contrib.seq2seq.sequence_loss(logits=logits, targets=target, weights=mask)\n",
    "train_opt = tf.train.AdamOptimizer(learning_rate=l_rate).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "10e06e2e1e0431f98d2d6e66f5afea442bab95f0"
   },
   "outputs": [],
   "source": [
    "# gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "saver = tf.train.Saver()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5b1055d71e4105670d1e36ac069d40df656a11ac",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Training\n",
    "display_step = 300\n",
    "total_loss = []\n",
    "total_acc = []\n",
    "\n",
    "test_total_loss = []\n",
    "test_total_acc = []\n",
    "for step in range(training_steps):\n",
    "    batch_loss = []\n",
    "    display_loss = []\n",
    "    batch_acc = []\n",
    "    display_acc = []\n",
    "    \n",
    "    test_batch_loss = []\n",
    "    test_display_loss = []\n",
    "    test_batch_acc = []\n",
    "    test_display_acc = []\n",
    "    \n",
    "    for mini_batch in range(dataset_size//batch_size):\n",
    "        source_batch, _ = add_padding_in_batch(x_train, mini_batch,mini_batch+batch_size, source_word_to_int)\n",
    "        source_batch = reverse_encoder_data(source_batch)\n",
    "\n",
    "        target_batch, batch_target_length = add_padding_in_batch(y_train, mini_batch,mini_batch+batch_size, target_word_to_int)\n",
    "\n",
    "        source_batch, target_batch, batch_target_length = shuffle(source_batch, target_batch, batch_target_length)\n",
    "#         print(np.array(target_batch).shape)\n",
    "        _,_loss = sess.run([train_opt,loss], feed_dict={encoder_input:source_batch, \n",
    "                                                        target:target_batch, \n",
    "                                                        target_length:np.array(batch_target_length), \n",
    "                                                        keep_ratio:keep_prob})\n",
    "        Infer_pred = sess.run(predictions, feed_dict={encoder_input:source_batch, target_length:[30]*batch_size, keep_ratio:1.0})\n",
    "        train_acc = get_accuracy(logits=np.array(Infer_pred), target=np.array(target_batch))\n",
    "        batch_acc.append(train_acc)\n",
    "        display_acc.append(train_acc)\n",
    "        total_acc.append(train_acc)\n",
    "        batch_loss.append(_loss)\n",
    "        display_loss.append(_loss)\n",
    "        total_loss.append(_loss)\n",
    "        if mini_batch % display_step == 0 :\n",
    "            print(\"Epoch : \", step+1, \" MiniBatch : \",mini_batch,\"/\",dataset_size//batch_size, \" Loss : \",np.mean(np.array(display_loss)), \" Accuracy : \",np.mean(np.array(batch_acc)))\n",
    "        \n",
    "        \n",
    "    for test_mini_batch in range(len(x_test)//batch_size):\n",
    "        test_source_batch, _ = add_padding_in_batch(x_test, mini_batch,mini_batch+batch_size, source_word_to_int)\n",
    "        test_source_batch = reverse_encoder_data(test_source_batch)\n",
    "\n",
    "        test_target_batch, test_batch_target_length = add_padding_in_batch(y_test, mini_batch,mini_batch+batch_size, target_word_to_int)\n",
    "\n",
    "        test_source_batch, test_target_batch, test_batch_target_length = shuffle(test_source_batch, test_target_batch, test_batch_target_length)\n",
    "        test_loss = sess.run(loss, feed_dict={encoder_input:test_source_batch, \n",
    "                                                        target:test_target_batch, \n",
    "                                                        target_length:np.array(test_batch_target_length), \n",
    "                                                        keep_ratio:1.0})\n",
    "        test_Infer_pred = sess.run(predictions, feed_dict={encoder_input:test_source_batch, target_length:[30]*batch_size, keep_ratio:1.0})\n",
    "        test_acc = get_accuracy(logits=np.array(test_Infer_pred), target=np.array(test_target_batch))\n",
    "        test_batch_acc.append(test_acc)\n",
    "        test_display_acc.append(test_acc)\n",
    "        test_total_acc.append(test_acc)\n",
    "        test_batch_loss.append(test_loss)\n",
    "        test_display_loss.append(test_loss)\n",
    "        test_total_loss.append(test_loss)\n",
    "        if test_mini_batch % display_step == 0 :\n",
    "            print(\"Epoch : \", step+1, \" MiniBatch : \",test_mini_batch,\"/\",len(x_test)//batch_size, \" Loss : \",np.mean(np.array(test_display_loss)), \" Accuracy : \",np.mean(np.array(test_display_acc)))\n",
    "    \n",
    "    print(\"Epoch : \", step+1, \" Batch Train Average Loss : \", np.mean(np.array(batch_loss)), \" Batch Train Average Accuracy : \", np.mean(np.array(batch_acc)))\n",
    "    print(\"Epoch : \", step+1, \" Batch Test Average Loss : \", np.mean(np.array(test_batch_loss)), \" Batch Test Average Accuracy : \", np.mean(np.array(test_batch_acc)))\n",
    "    start_num, end_num = 1000, 1050\n",
    "    for i in range(start_num, end_num):\n",
    "        translate_sentence = source_data.split(\"\\n\")[i].split()\n",
    "        translate_sequence = [source_word_to_int[word] for word in translate_sentence]\n",
    "        translate_sequence = reverse_encoder_data([translate_sequence]*batch_size)\n",
    "\n",
    "        translate_logits = sess.run(predictions, feed_dict={encoder_input:translate_sequence,\n",
    "                                                            target_length:[len(translate_sequence)*2]*batch_size, \n",
    "                                                            keep_ratio:1.0})[0]\n",
    "        print(\"Source : \",source_data.split(\"\\n\")[i])\n",
    "        print(\"Target : \",target_data.split(\"\\n\")[i])\n",
    "        print(\"Predicted : \",\" \".join([target_int_to_word[idx] for idx in translate_logits]))\n",
    "print( \" Total Train Loss : \", np.mean(np.array(total_loss)), \" \\nTotal Train Accuracy : \", np.mean(np.array(total_acc)))\n",
    "print( \" Total Train Loss : \", np.mean(np.array(test_loss)), \" \\nTotal Test Accuracy : \", np.mean(np.array(test_total_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "165f9501dc0527e70a6edaec890c96e40268ab56",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "saver.restore(sess,\"weights/weights.ckpt\")\n",
    "start_num, end_num = 1000, 1050\n",
    "for i in range(start_num, end_num):\n",
    "    translate_sentence = source_data.split(\"\\n\")[i].split()\n",
    "    translate_sequence = [source_word_to_int[word] for word in translate_sentence]\n",
    "#     translate_sequence = reverse_encoder_data([translate_sequence]*batch_size)\n",
    "    \n",
    "    translate_logits = sess.run(predictions, feed_dict={encoder_input:[translate_sequence]*batch_size,\n",
    "                                                        target_length:[len(translate_sequence)*2]*batch_size, \n",
    "                                                        keep_ratio:1.0})[0]\n",
    "    print(\"Source : \",source_data.split(\"\\n\")[i])\n",
    "    print(\"Target : \",target_data.split(\"\\n\")[i])\n",
    "    print(\"Predicted : \",\" \".join([target_int_to_word[idx] for idx in translate_logits]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "512d4aba59187e218b7607a9e11aaaf59dbad639",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_num, end_num = 10, 60\n",
    "for i in range(start_num, end_num):\n",
    "    translate_sentence = source_data.split(\"\\n\")[i].split()\n",
    "    translate_sequence = [source_word_to_int[word] for word in translate_sentence]\n",
    "#     translate_sequence = reverse_encoder_data([translate_sequence]*batch_size)\n",
    "    \n",
    "    translate_logits = sess.run(predictions, feed_dict={encoder_input:[translate_sequence]*batch_size,\n",
    "                                                        target_length:[len(translate_sequence)*2]*batch_size, \n",
    "                                                        keep_ratio:1.0})[0]\n",
    "    print(\"Source : \",source_data.split(\"\\n\")[i])\n",
    "    print(\"Target : \",target_data.split(\"\\n\")[i])\n",
    "    print(\"Predicted : \",\" \".join([target_int_to_word[idx] for idx in translate_logits]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d06904984361afd5e2f9288e3293f8d185ee9232"
   },
   "outputs": [],
   "source": [
    "target_word_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "790963688a5affde029c777d0ae771d79de09429"
   },
   "outputs": [],
   "source": [
    "print(\"Source : \",source_data.split(\"\\n\")[0])\n",
    "print(\"Target : \",target_data.split(\"\\n\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a2f923f5ad54ec1aeeaf7441d92bcfdbf823aec7"
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "13b568e1eb81469abadce9e8f941f79485c297e5"
   },
   "outputs": [],
   "source": [
    "saver.save(sess, \"weights/weights_new.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d2d1582b4a93d754da52a16435da74ef8b00ac3d"
   },
   "outputs": [],
   "source": [
    "source_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "59a6ea8f3e7c75ebe8b51c345bd10f0177b2ae3b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
